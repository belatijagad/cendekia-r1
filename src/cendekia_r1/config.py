import os

HF_TOKEN_SECRET_NAME = os.environ.get('HUGGINGFACE_TOKEN')
WANDB_TOKEN_SECRET_NAME = os.environ.get('WANDB_API_KEY')

EXPERIMENT_NAME = 'cendekia-r1-zero-qwen-2.5-1.5b-instruct'
BASE_MODEL_NAME = "Qwen/Qwen2.5-1.5B-Instruct"
SAVED_LORA_NAME = "grpo_saved_lora"

MAX_SEQ_LENGTH = 4096
LORA_RANK = 16
LOAD_IN_4BIT = True
FAST_INFERENCE = True
GPU_MEMORY_UTILIZATION = 0.5

LORA_ALPHA = LORA_RANK
TARGET_MODULES = [
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj",
]
LORA_RANDOM_STATE = 69
USE_GRADIENT_CHECKPOINTING = "unsloth"

CSV_PATH = "data/exam_data.csv" # Adjusted path
QUESTION_COL = "Question"
OPTIONS_COL = "Options"
CORRECT_ANSWER_COL = "Answer"

SFT_RATIO = 0.3
GRPO_RATIO = 0.4

NUM_GENERATIONS = 8
MAX_PROMPT_LENGTH = 256
MAX_COMPLETION_LENGTH = MAX_SEQ_LENGTH - MAX_PROMPT_LENGTH

USE_VLLM = True
LEARNING_RATE = 5e-6
ADAM_BETA1 = 0.9
ADAM_BETA2 = 0.99
WEIGHT_DECAY = 0.1
WARMUP_RATIO = 0.1
LR_SCHEDULER_TYPE = "cosine"
OPTIM = "adamw_8bit"
LOGGING_STEPS = 1
PER_DEVICE_TRAIN_BATCH_SIZE = 1
GRADIENT_ACCUMULATION_STEPS = 4
NUM_TRAIN_EPOCHS = 7
MAX_STEPS = 50
SAVE_STEPS = 50
MAX_GRAD_NORM = 0.1
REPORT_TO = "wandb"
OUTPUT_DIR = "outputs"
LOG_COMPLETIONS = True